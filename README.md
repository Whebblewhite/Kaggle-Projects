# Kaggle-Projects

1. House Predictions
Ask a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.
With 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.

This project showcases skills in data wrangling and cleaning, feature transformation, model selection.
I use a combination of Linear Regression, Ridge Regression, ElasticNet, BayesianRidge, HuberRegressor models as well as more complex algorithms such as LGBM, Gradient Boosting,
SVR, CatboostRegressor and XGBRressor. 

2. Titanic Project 
This is the legendary Titanic ML competition â€“ the best, first challenge for you to dive into ML competitions and familiarize yourself with how the Kaggle platform works.The competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.

This project showcases skills in data wrangling and cleaning, feature transformation, model selection. 
I use a variety of models such as Linear Regression, Decision Tree, KNN, Random Forest Classifier, Support Vector Classifier and XGB. I then use a voting
classifier hard nad soft to take the most high performing of those algorithms. 
